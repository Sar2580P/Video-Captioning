# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h-hepNcTTf7m4fpwnbyOuHcyvgFO-anC
"""

# -*- coding: utf-8 -*-
"""PipeLine_Attention_utilities.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q8tfAY5estZTXYmgdGAToeXX3Zau-HHP
"""

import os
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras

features_2d = '/content/drive/MyDrive/Video_Cap_Data/features_2d'
features_3d = '/content/drive/MyDrive/Video_Cap_Data/features_3d'


def load_npy(video_id , cap, path_2d= features_2d , path_3d= features_3d):
  vid_id = str(video_id, 'UTF-8')
  vgg_features_path = os.path.join(path_2d , vid_id[5:])
  movinet_features_path = os.path.join(path_3d , vid_id)

  tensor_2d = np.load(vgg_features_path + '.npy')
  tensor_3d = np.load(movinet_features_path + '.npy')

  sz = tensor_2d.shape
  if sz[0]!=21:
    diff_2d = 20- sz[0]
    a = tf.zeros((diff_2d, 4096))
    tensor_2d = tf.concat([a, tensor_2d], 0)

  sz = tensor_3d.shape
  if sz[0]!=5:
    diff_3d = 5- sz[0]
    a = tf.zeros((diff_3d, 480))
    tensor_3d = tf.concat([a, tensor_3d], 0)

  return tensor_2d, tensor_3d, cap

########################################################################

def create_dataset(img_name,caption, batch_sz, buffer_sz):

  dataset = tf.data.Dataset.from_tensor_slices((img_name , caption))

  dataset = dataset.map (lambda item1, item2: tf.numpy_function(load_npy, [item1, item2] , [tf.float32,tf.float32, tf.int32]), 
                                              num_parallel_calls=tf.data.experimental.AUTOTUNE)
  # Shuffle and batch
  dataset = dataset.batch(batch_sz).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
  return dataset

########################################################################

# https://www.tensorflow.org/tutorials/text/image_captioning

class Encoder(tf.keras.Model):

    # This encoder passes the features through a Fully connected layer
    def __init__(self, embedding_dim):
        super(Encoder, self).__init__()
        
      
        # tf.keras.layers.Dropout(0.5)
        self.fc = tf.keras.layers.Dense(embedding_dim, activation='relu')
        
    
    def call(self, x):
        return self.fc(x)

########################################################################



class Attention(tf.keras.layers.Layer):
  def __init__(self, units, embedding_dim):
    super(Attention, self).__init__()

    initializer = tf.random_normal_initializer(mean=1., stddev=2.)
    self.scaling = tf.Variable(initializer(shape=[embedding_dim], dtype=tf.float32))
    
    # Implementing Attention Mechanism 
    self.attention = tf.keras.layers.MultiHeadAttention(num_heads = 2, key_dim=embedding_dim)

  def call(self,vector, features):
        
    context_vector = self.attention(query= vector, value =features )
    # context_vector = tf.reduce_sum(context_vector, axis=1)    # (16,256)
    
    context_vector = tf.math.multiply(self.scaling, context_vector)
    return context_vector

#########################################################################

class MFATT(tf.keras.layers.Layer):
  def __init__(self, units):
    super(MFATT, self).__init__()
    self.fc = tf.keras.layers.Dense(units, activation = 'relu')

  def call(self , context_2d, context_3d , x):
    context_vector = tf.concat([context_2d, context_3d], axis=-1)
    x = tf.concat([x ,context_vector], axis=-1)

    return self.fc(x)

########################################################################
class Output(tf.keras.layers.Layer):
  def __init__(self, BATCH_SIZE, units):
    super(Output, self).__init__()

    self.lstm = tf.keras.layers.LSTMCell(units)

  def call(self ,m_x, h, c):
    # m_x = tf.expand_dims(m_x , 1)     # (16, 1, embedding_dims)
    # self.lstm.states[0] = h
    # self.lstm.states[1] = c
    m_x = tf.squeeze(m_x, axis=1)
    _, (h_, c_) = self.lstm(inputs = m_x, states = [h,c])

    return  h_, c_
                          
#########################################################################
class Decoder(tf.keras.Model):
  def __init__(self, tokenizer, BATCH_SIZE, units, vocab_size, embedding_dim):
    super(Decoder, self).__init__()

    self.Embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)


    self.get_context_x = Attention(units, embedding_dim)
    self.get_context_h = Attention(units, embedding_dim)

    self.mfatt_x = MFATT(units)
    self.mfatt_h = MFATT(units)
    self.vocab_size = vocab_size
    self.output_ = Output(BATCH_SIZE, units)

    self.fc = TokenOutput(tokenizer, vocab_size, banned_tokens=(' ' , '<unk>', '<start>'))
    # This might run a little faster if the dataset didn't also have to load the image data.
    self.fc.adapt()
    # self.output_layer = output_layer

    self.tokenizer = tokenizer

  def call(self, x, features_2d, features_3d, hidden, cell):

    x = self.Embedding(x)

    context_2d_x = self.get_context_x(x ,features_2d)
    context_3d_x = self.get_context_x(x ,features_3d)

    m_x = self.mfatt_x(context_2d_x , context_3d_x , x)
    hidden_state, cell_state = self.output_(m_x, hidden ,cell)

    h = tf.expand_dims(hidden_state, axis=1)

    context_2d_h = self.get_context_h(h, features_2d)
    context_3d_h = self.get_context_h(h, features_3d)
    m_h = self.mfatt_h(context_2d_h, context_3d_h , h)

    # E = self.Embedding.weights
    # print('Embedding : ' , E.shape)
    
    # p_t1 = tf.nn.softmax(tf.matmul(m_h, tf.transpose(E[0])) , axis = 1)
    p_t1 = self.fc(m_h)
    return p_t1, hidden_state, cell_state


########################################################################
class TokenOutput(tf.keras.layers.Layer):
  def __init__(self, tokenizer,vocab_size , banned_tokens=(' ', '<unk>', '<start>'), **kwargs):
    super().__init__()


    self.vocab_size = vocab_size
    self.dense = tf.keras.layers.Dense(
        units=vocab_size, **kwargs)
    self.tokenizer = tokenizer
    self.banned_tokens = banned_tokens

    self.bias = None

#______________________________________________________________________________________________________________

  def adapt(self):
    counts_arr = np.zeros(shape=(self.vocab_size,))

    for word, ct in self.tokenizer.word_counts.items():
      idx = self.tokenizer.word_index[word]
      counts_arr[idx] = ct

    # setting freq of bad_tokens = 0
    for word in self.banned_tokens:
      idx = self.tokenizer.word_index[word]
      counts_arr[idx] = 0

    # calculating prob. distr. over words in vocab
    total = counts_arr.sum()
    p = counts_arr/total

    # calculating entropy of vocab_distr
    p[counts_arr==0] = 1.0
    log_p = np.log(p)  # log(1) == 0
    entropy = -(log_p*p).sum()

    print()
    print(f"Uniform entropy: {np.log(self.vocab_size):0.2f}")
    print(f"Marginal entropy: {entropy:0.2f}")

    self.bias = log_p
    self.bias[counts_arr==0] = -1e9         # setting very -ve bias for banned tokens

#___________________________________________________________________________________________________________________

  def call(self, x):
    x = self.dense(x)
    # An Add layer doesn't work because of the different shapes.
    # This clears the mask, that's okay because it prevents keras from rescaling
    # the losses.
    return x + self.bias





