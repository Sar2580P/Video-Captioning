# -*- coding: utf-8 -*-
"""text_utilities.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rZEgi2AIRsXhRxHSJReK2DQxnQBJVZXL
"""

import string
import pandas as pd
from matplotlib import pyplot as plt
from tensorflow.keras.preprocessing.sequence import pad_sequences



# To remove punctuations
def remove_punctuation(text_original):
    text_no_punctuation = text_original.translate(string.punctuation)
    return(text_no_punctuation)


# To remove single characters
def remove_single_character(text):
    text_len_more_than1 = ""
    for word in text.split():
        if len(word) > 1:
            text_len_more_than1 += " " + word
    return(text_len_more_than1)

# To remove numeric values
def remove_numeric(text,printTF=False):
    text_no_numeric = ""
    for word in text.split():
        isalpha = word.isalpha()      # returns True if all the characters are alphabet letters (a-z).
        if printTF:
            print("    {:10} : {:}".format(word,isalpha))
        if isalpha:
            text_no_numeric += " " + word
    return(text_no_numeric)

def text_clean(df, col:int):
  new_captions = []
  for i in range(len(df)):
    text_original = df.iloc[i ,col]
    text = remove_punctuation(text_original)
    text = remove_single_character(text)
    text = remove_numeric(text)
    
    s = '<start> '
    s += text 
    s += ' <end>'
    new_captions.append(s)

  return new_captions

def create_vocabulary(df, col:int):
  vocab = []
  max_len = 0 
  for i in range(len(df)):
    cap = df.iloc[i,col]
    x = cap.split()
    max_len = max(max_len , len(x))
    vocab.extend(x)      # adds the specified list ele (or any iterable) to end of curr_list.

  vocab_size = len(set(vocab))
  return vocab , vocab_size, max_len

########################################################

def df_word_count(vocab):
    data = {}

    for word in vocab:
      if word not in data:
        data[word] = 0
      data[word] +=1

    dfword = pd.DataFrame(list(data.items()) , columns = ['word' , 'count'])
    dfword = dfword.sort_values(by='count', ascending=False)
    dfword = dfword.reset_index()[["word","count"]]

    return dfword



def plt_histo(vocab ,topn = 75):
    df = df_word_count(vocab)
    df = df.iloc[:topn, :]
    title = "The top {topn} most frequently appearing words".format(topn = topn)
    plt.figure(figsize=(30,3))
    plt.bar(df.index,df["count"],color ='g')
    plt.yticks(fontsize=20,color ='r')
    plt.xticks(df.index,df["word"],rotation=90,fontsize=20,color ='r')
    plt.title(title,fontsize=20)
    plt.show()


# plt_histo(dfwordcount.iloc[-topn:,:],
        # title="The least 50 most frequently appearing words")

def get_seqs(tokenizer, captions, max_len):
  # Create the tokenized vectors
  seqs = tokenizer.texts_to_sequences(captions)
  seqs = pad_sequences(seqs, maxlen= max_len, padding='post')
  return seqs